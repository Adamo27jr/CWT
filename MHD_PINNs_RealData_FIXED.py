
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
import scipy.io as sio
import pywt
from sklearn.preprocessing import StandardScaler
import time
import os
import glob

class UltraRobust_RealDataLoader:
    """
    Chargeur ULTRA-ROBUSTE pour les vraies donn√©es Kelvin-Helmholtz
    """
    def __init__(self, data_path="C:/Users/user/Desktop/Donnees_KVI"):
        self.data_path = data_path
        self.cases = {
            'B_0': 'Kelvin_Helmholtz_Instabilities_B_0',
            'B_0_1': 'Kelvin_Helmholtz_Instabilities_B_0_1', 
            'B_0_2': 'Kelvin_Helmholtz_Instabilities_B_0_2',
            'B_0_05': 'Kelvin_Helmholtz_Instabilities_B_0_05'
        }
        self.loaded_data = {}

    def analyze_matlab_structure(self, mat_file):
        """
        Analyse la structure du fichier MATLAB pour identifier les vrais champs
        """
        try:
            data = sio.loadmat(mat_file)
            available_keys = [k for k in data.keys() if not k.startswith('__')]

            print(f"      üîç ANALYSE STRUCTURE:")
            field_info = {}

            for key in available_keys:
                value = data[key]
                if isinstance(value, np.ndarray):
                    field_info[key] = {
                        'shape': value.shape,
                        'dtype': value.dtype,
                        'is_2d': value.ndim == 2 and min(value.shape) > 1,
                        'is_scalar': value.size == 1,
                        'size': value.size
                    }
                    print(f"         {key}: {value.shape} ({value.dtype}) - {'2D' if field_info[key]['is_2d'] else 'Scalaire' if field_info[key]['is_scalar'] else 'Autre'}")

            return data, field_info

        except Exception as e:
            print(f"      ‚ùå Erreur analyse: {e}")
            return None, {}

    def smart_field_mapping(self, data, field_info):
        """
        Mapping intelligent bas√© sur les dimensions et le contenu
        """
        fields = {}

        # Trouver les champs 2D (vrais champs physiques)
        field_2d = []
        for key, info in field_info.items():
            if info['is_2d'] and info['size'] > 1000:  # Champs suffisamment grands
                field_2d.append((key, info['shape'], info['size']))

        # Trier par taille (les plus grands en premier)
        field_2d.sort(key=lambda x: x[2], reverse=True)

        print(f"      üéØ CHAMPS 2D IDENTIFI√âS: {len(field_2d)}")
        for key, shape, size in field_2d:
            print(f"         {key}: {shape} ({size} points)")

        # Mapping intelligent bas√© sur les noms ET les dimensions
        if len(field_2d) >= 4:  # Au moins 4 champs 2D

            # Strat√©gie 1: Mapping par nom si possible
            name_mapping = {
                'u1': 'vx', 'u2': 'vy',  # Vitesses
                'B1': 'Bx', 'B2': 'By',  # Champ magn√©tique
                'w': 'vorticity',        # Vorticit√©
                'J': 'current',          # Densit√© de courant
                'rho': 'density',        # Densit√©
                'p': 'pressure'          # Pression
            }

            for key, shape, size in field_2d:
                if key in name_mapping:
                    fields[name_mapping[key]] = data[key]
                    print(f"      ‚úÖ {name_mapping[key]} <- {key}: {shape}")

            # Strat√©gie 2: Si pas assez de champs nomm√©s, utiliser l'ordre
            if len(fields) < 4:
                print(f"      üîÑ FALLBACK: Mapping par ordre de taille")
                field_names = ['vx', 'vy', 'Bx', 'By', 'vorticity', 'current']

                for i, (key, shape, size) in enumerate(field_2d[:6]):
                    if i < len(field_names):
                        fields[field_names[i]] = data[key]
                        print(f"      ‚úÖ {field_names[i]} <- {key}: {shape}")

        # Calculer la vorticit√© si on a vx et vy
        if 'vx' in fields and 'vy' in fields:
            try:
                vx, vy = fields['vx'], fields['vy']
                if vx.shape == vy.shape:
                    dvx_dy = np.gradient(vx, axis=0)
                    dvy_dx = np.gradient(vy, axis=1)
                    fields['vorticity_computed'] = dvy_dx - dvx_dy
                    print(f"      ‚úÖ vorticity_computed calcul√©e: {fields['vorticity_computed'].shape}")
            except Exception as e:
                print(f"      ‚ö†Ô∏è Erreur calcul vorticit√©: {e}")

        return fields

    def load_case_data(self, case_name, max_files=5):
        """
        Charge les donn√©es d'un cas sp√©cifique - VERSION ULTRA-ROBUSTE
        """
        print(f"üîÑ CHARGEMENT DES DONN√âES: {case_name}")

        case_path = os.path.join(self.data_path, self.cases[case_name])

        # Chercher les sous-dossiers
        subdirs = []
        if os.path.exists(case_path):
            subdirs = [d for d in os.listdir(case_path) if os.path.isdir(os.path.join(case_path, d))]

        case_data = {}

        for subdir in subdirs:
            subdir_path = os.path.join(case_path, subdir)

            # Patterns de fichiers plus flexibles
            patterns = [
                os.path.join(subdir_path, "*.mat"),
                os.path.join(subdir_path, "MHD_data_*.mat"),
                os.path.join(subdir_path, f"*{case_name}*.mat")
            ]

            mat_files = []
            for pattern in patterns:
                mat_files.extend(glob.glob(pattern))

            # Supprimer les doublons et trier
            mat_files = sorted(list(set(mat_files)))[:max_files]

            subdir_data = []
            for mat_file in mat_files:
                try:
                    print(f"   üìÇ Lecture: {os.path.basename(mat_file)}")

                    # Analyse structure
                    data, field_info = self.analyze_matlab_structure(mat_file)
                    if data is None:
                        continue

                    # Mapping intelligent
                    fields = self.smart_field_mapping(data, field_info)

                    if not fields:
                        print(f"      ‚ùå Aucun champ 2D trouv√©")
                        continue

                    # V√©rifier la coh√©rence des dimensions
                    shapes = [v.shape for v in fields.values() if isinstance(v, np.ndarray)]
                    if len(set(shapes)) > 1:
                        print(f"      ‚ö†Ô∏è Dimensions incoh√©rentes: {set(shapes)}")
                        # Garder seulement les champs de m√™me dimension
                        main_shape = max(set(shapes), key=shapes.count)
                        fields = {k: v for k, v in fields.items() 
                                if isinstance(v, np.ndarray) and v.shape == main_shape}
                        print(f"      üîß Filtr√© vers dimension principale: {main_shape}")

                    # M√©tadonn√©es
                    fields['file'] = os.path.basename(mat_file)
                    fields['time'] = self.extract_time_from_filename(mat_file)
                    fields['shape'] = list(fields.values())[0].shape if fields else None

                    if fields:
                        subdir_data.append(fields)
                        print(f"      ‚úÖ {len(fields)-3} champs extraits avec succ√®s")

                except Exception as e:
                    print(f"      ‚ùå Erreur: {e}")
                    continue

            if subdir_data:
                case_data[subdir] = subdir_data
                print(f"   ‚úÖ {subdir}: {len(subdir_data)} fichiers trait√©s")

        self.loaded_data[case_name] = case_data
        return case_data

    def extract_time_from_filename(self, filename):
        """
        Extrait le temps du nom de fichier
        """
        import re
        match = re.search(r't(\d+(?:\.\d+)?)', filename)
        return float(match.group(1)) if match else 0.0

    def get_training_data(self, case_name, subdir=None, time_steps=None):
        """
        Pr√©pare les donn√©es pour l'entra√Ænement PINNs - VERSION ULTRA-ROBUSTE
        """
        if case_name not in self.loaded_data:
            self.load_case_data(case_name)

        case_data = self.loaded_data[case_name]

        if not case_data:
            print(f"‚ùå Aucune donn√©e disponible pour {case_name}")
            return []

        # S√©lectionner le sous-dossier
        if subdir is None:
            subdir = list(case_data.keys())[0]

        if subdir not in case_data:
            print(f"‚ùå Sous-dossier {subdir} non trouv√©")
            return []

        data_list = case_data[subdir]

        if not data_list:
            print(f"‚ùå Aucune donn√©e dans {subdir}")
            return []

        # S√©lectionner les pas de temps
        if time_steps is not None:
            data_list = data_list[:time_steps]

        # Convertir en tenseurs PyTorch
        training_data = []

        for data_dict in data_list:
            try:
                # Trouver un champ de r√©f√©rence pour les dimensions
                ref_field = None
                ref_key = None

                # Priorit√© aux champs physiques
                priority_keys = ['vx', 'vy', 'Bx', 'By', 'vorticity', 'current']

                for key in priority_keys:
                    if key in data_dict and isinstance(data_dict[key], np.ndarray) and data_dict[key].ndim == 2:
                        ref_field = data_dict[key]
                        ref_key = key
                        break

                # Si pas trouv√©, prendre le premier champ 2D
                if ref_field is None:
                    for key, value in data_dict.items():
                        if isinstance(value, np.ndarray) and value.ndim == 2 and value.size > 1000:
                            ref_field = value
                            ref_key = key
                            break

                if ref_field is None:
                    print(f"‚ö†Ô∏è Aucun champ de r√©f√©rence 2D trouv√©")
                    continue

                print(f"      üéØ Champ de r√©f√©rence: {ref_key} {ref_field.shape}")

                # Cr√©er la grille de coordonn√©es
                ny, nx = ref_field.shape
                x = np.linspace(-1, 1, nx)
                y = np.linspace(-1, 1, ny)
                X, Y = np.meshgrid(x, y)
                coords = np.stack([X.flatten(), Y.flatten()], axis=1)

                # Pr√©parer les champs (seulement ceux de m√™me dimension)
                fields = {}

                # Chercher les paires de champs vectoriels
                if 'vx' in data_dict and 'vy' in data_dict:
                    vx_data = data_dict['vx']
                    vy_data = data_dict['vy']
                    if vx_data.shape == ref_field.shape and vy_data.shape == ref_field.shape:
                        vx_flat = vx_data.flatten()
                        vy_flat = vy_data.flatten()
                        fields['velocity'] = np.stack([vx_flat, vy_flat], axis=1)
                        print(f"      ‚úÖ velocity: {fields['velocity'].shape}")

                if 'Bx' in data_dict and 'By' in data_dict:
                    Bx_data = data_dict['Bx']
                    By_data = data_dict['By']
                    if Bx_data.shape == ref_field.shape and By_data.shape == ref_field.shape:
                        Bx_flat = Bx_data.flatten()
                        By_flat = By_data.flatten()
                        fields['magnetic'] = np.stack([Bx_flat, By_flat], axis=1)
                        print(f"      ‚úÖ magnetic: {fields['magnetic'].shape}")

                # Champs scalaires
                scalar_fields = ['vorticity', 'vorticity_computed', 'current', 'density', 'pressure']
                for field_name in scalar_fields:
                    if field_name in data_dict:
                        field_data = data_dict[field_name]
                        if isinstance(field_data, np.ndarray) and field_data.shape == ref_field.shape:
                            fields[field_name] = field_data.flatten().reshape(-1, 1)
                            print(f"      ‚úÖ {field_name}: {fields[field_name].shape}")

                # Convertir en tenseurs
                coords_tensor = torch.FloatTensor(coords)
                fields_tensor = {}
                for key, value in fields.items():
                    if isinstance(value, np.ndarray):
                        fields_tensor[key] = torch.FloatTensor(value)

                if fields_tensor:  # Seulement si on a des champs
                    training_data.append({
                        'coords': coords_tensor,
                        'fields': fields_tensor,
                        'time': data_dict['time'],
                        'file': data_dict['file']
                    })
                    print(f"      ‚úÖ Donn√©es pr√©par√©es: {coords_tensor.shape[0]} points, {len(fields_tensor)} champs")

            except Exception as e:
                print(f"      ‚ùå Erreur pr√©paration donn√©es: {e}")
                import traceback
                traceback.print_exc()
                continue

        print(f"üìä R√âSUM√â DONN√âES D'ENTRA√éNEMENT:")
        print(f"   ‚Ä¢ Cas: {case_name}")
        print(f"   ‚Ä¢ Sous-dossier: {subdir}")
        print(f"   ‚Ä¢ Pas de temps: {len(training_data)}")
        if training_data:
            print(f"   ‚Ä¢ Points par pas: {training_data[0]['coords'].shape[0]}")
            print(f"   ‚Ä¢ Champs disponibles: {list(training_data[0]['fields'].keys())}")

        return training_data

class MorletWaveletLayer(nn.Module):
    """
    Couche d'ondelettes Morlet int√©gr√©e dans le r√©seau
    """
    def __init__(self, input_dim, n_scales=15, trainable_scales=True):
        super().__init__()
        self.input_dim = input_dim
        self.n_scales = n_scales

        # √âchelles d'ondelettes (param√®tres apprenables)
        if trainable_scales:
            self.scales = nn.Parameter(torch.linspace(1, 20, n_scales))
        else:
            self.register_buffer('scales', torch.linspace(1, 20, n_scales))

        # Param√®tres Morlet apprenables
        self.omega0 = nn.Parameter(torch.tensor(6.0))  # Fr√©quence centrale
        self.sigma = nn.Parameter(torch.tensor(1.0))   # Largeur

    def morlet_wavelet(self, t, scale):
        """Ondelette Morlet complexe"""
        # Normalisation
        norm = 1.0 / torch.sqrt(scale)

        # Argument normalis√©
        arg = t / scale

        # Ondelette Morlet complexe
        envelope = torch.exp(-0.5 * (arg / self.sigma)**2)
        oscillation = torch.exp(1j * self.omega0 * arg)

        return norm * envelope * oscillation

    def forward(self, x):
        """
        Transform√©e en ondelettes continue pour chaque signal
        x: (batch_size, signal_length)
        """
        batch_size, signal_length = x.shape

        # Cr√©er la grille temporelle
        t = torch.linspace(-signal_length//2, signal_length//2, signal_length, device=x.device)

        # Coefficients d'ondelettes pour toutes les √©chelles
        coeffs = []

        for scale in self.scales:
            # Ondelette m√®re √† cette √©chelle
            wavelet = self.morlet_wavelet(t, scale)

            # Convolution (transform√©e en ondelettes)
            # Utiliser la partie r√©elle pour la suite
            wavelet_real = wavelet.real.unsqueeze(0).unsqueeze(0)  # (1, 1, signal_length)
            x_expanded = x.unsqueeze(1)  # (batch_size, 1, signal_length)

            # Convolution 1D
            coeff = torch.nn.functional.conv1d(
                x_expanded, 
                wavelet_real, 
                padding=signal_length//2
            )[:, 0, :signal_length]  # (batch_size, signal_length)

            coeffs.append(coeff)

        # Empiler tous les coefficients
        wavelet_coeffs = torch.stack(coeffs, dim=1)  # (batch_size, n_scales, signal_length)

        return wavelet_coeffs

class UltraRobust_MHD_PINNs(nn.Module):
    """
    Physics-Informed Neural Network pour MHD avec vraies donn√©es Kelvin-Helmholtz - ULTRA-ROBUSTE
    """
    def __init__(self, input_dim=2, hidden_dims=[128, 128, 64], n_scales=15):
        super().__init__()

        # Couche d'ondelettes Morlet
        self.wavelet_layer = MorletWaveletLayer(input_dim, n_scales)
        self.n_scales = n_scales

        # R√©seau principal pour les champs MHD
        layers = []
        current_dim = input_dim  # (x, y) coordinates

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.Tanh(),  # Tanh pour PINNs (meilleure stabilit√©)
            ])
            current_dim = hidden_dim

        self.main_network = nn.Sequential(*layers)

        # T√™tes de sortie pour les champs MHD
        self.velocity_head = nn.Sequential(
            nn.Linear(current_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 2)  # (vx, vy)
        )

        self.magnetic_head = nn.Sequential(
            nn.Linear(current_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 2)  # (Bx, By)
        )

        self.pressure_head = nn.Sequential(
            nn.Linear(current_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1)  # p
        )

        self.density_head = nn.Sequential(
            nn.Linear(current_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1)  # œÅ
        )

        # R√©seau pour pr√©diction des singularit√©s bas√© sur ondelettes
        wavelet_feature_dim = 3 * n_scales  # mean, std, max pour chaque √©chelle

        self.singularity_network = nn.Sequential(
            nn.Linear(wavelet_feature_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 3)  # (n_singularities, max_intensity, total_energy)
        )

    def forward(self, coords):
        """
        coords: (batch_size, 2) - coordonn√©es (x, y)
        """
        # Extraction des features du r√©seau principal
        features = self.main_network(coords)

        # Pr√©diction des champs MHD
        velocity = self.velocity_head(features)      # (vx, vy)
        magnetic = self.magnetic_head(features)      # (Bx, By)
        pressure = self.pressure_head(features)      # p
        density = self.density_head(features)        # œÅ

        return {
            'velocity': velocity,
            'magnetic': magnetic,
            'pressure': pressure,
            'density': density,
            'features': features
        }

    def predict_singularities(self, field_data):
        """
        Pr√©dire les singularit√©s √† partir des donn√©es de champ
        field_data: (batch_size, signal_length) - donn√©es 1D du champ
        """
        # Analyse par ondelettes Morlet
        wavelet_coeffs = self.wavelet_layer(field_data)  # (batch_size, n_scales, signal_length)

        # Statistiques des coefficients d'ondelettes par √©chelle
        mean_coeffs = torch.mean(torch.abs(wavelet_coeffs), dim=2)  # (batch_size, n_scales)
        std_coeffs = torch.std(torch.abs(wavelet_coeffs), dim=2)    # (batch_size, n_scales)
        max_coeffs = torch.max(torch.abs(wavelet_coeffs), dim=2)[0] # (batch_size, n_scales)

        # Concat√©ner toutes les features: 3 * n_scales
        wavelet_features = torch.cat([mean_coeffs, std_coeffs, max_coeffs], dim=1)

        # Pr√©diction des singularit√©s
        singularity_pred = self.singularity_network(wavelet_features)

        return {
            'n_singularities': singularity_pred[:, 0],
            'max_intensity': singularity_pred[:, 1],
            'total_energy': singularity_pred[:, 2],
            'wavelet_coeffs': wavelet_coeffs
        }

class UltraRobust_MHD_Trainer:
    """
    Entra√Æneur pour le r√©seau PINNs MHD avec vraies donn√©es - VERSION ULTRA-ROBUSTE
    """
    def __init__(self, model, data_loader):
        self.model = model
        self.data_loader = data_loader

        # Optimizers
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)

        # Historique des losses
        self.loss_history = {
            'total': [], 'data': [], 'singularity': []
        }

    def compute_real_data_loss(self, coords, target_fields):
        """
        Calcule la loss bas√©e sur les vraies donn√©es
        """
        outputs = self.model(coords)

        total_loss = 0.0
        loss_details = {}
        n_losses = 0

        # Loss pour chaque champ disponible
        if 'velocity' in target_fields and 'velocity' in outputs:
            loss_velocity = torch.mean((outputs['velocity'] - target_fields['velocity'])**2)
            total_loss += loss_velocity
            loss_details['velocity'] = loss_velocity.item()
            n_losses += 1

        if 'magnetic' in target_fields and 'magnetic' in outputs:
            loss_magnetic = torch.mean((outputs['magnetic'] - target_fields['magnetic'])**2)
            total_loss += loss_magnetic
            loss_details['magnetic'] = loss_magnetic.item()
            n_losses += 1

        if 'pressure' in target_fields and 'pressure' in outputs:
            loss_pressure = torch.mean((outputs['pressure'] - target_fields['pressure'])**2)
            total_loss += loss_pressure
            loss_details['pressure'] = loss_pressure.item()
            n_losses += 1

        if 'density' in target_fields and 'density' in outputs:
            loss_density = torch.mean((outputs['density'] - target_fields['density'])**2)
            total_loss += loss_density
            loss_details['density'] = loss_density.item()
            n_losses += 1

        # Moyenner si plusieurs losses
        if n_losses > 0:
            total_loss = total_loss / n_losses

        return total_loss, loss_details

    def compute_singularity_loss_from_real_data(self, real_vorticity):
        """
        Calcule la loss des singularit√©s √† partir de vraies donn√©es de vorticit√©
        """
        if real_vorticity is None or len(real_vorticity) == 0:
            return torch.tensor(0.0), {}

        # Analyser les vraies donn√©es pour extraire les singularit√©s
        if real_vorticity.dim() > 1:
            vorticity_1d = real_vorticity.flatten()
        else:
            vorticity_1d = real_vorticity

        # D√©tection simple des singularit√©s (pics dans la vorticit√©)
        vorticity_np = vorticity_1d.detach().cpu().numpy()

        # Trouver les pics
        try:
            from scipy.signal import find_peaks
            peaks, properties = find_peaks(np.abs(vorticity_np), height=np.std(vorticity_np))

            # Targets bas√©s sur les vraies donn√©es
            n_singularities_target = len(peaks)
            max_intensity_target = np.max(np.abs(vorticity_np)) if len(vorticity_np) > 0 else 0.0
            total_energy_target = np.sum(vorticity_np**2)

            # Pr√©diction du mod√®le
            vorticity_tensor = vorticity_1d.unsqueeze(0)  # (1, signal_length)
            singularity_pred = self.model.predict_singularities(vorticity_tensor)

            # Loss
            loss_n_sing = (singularity_pred['n_singularities'][0] - n_singularities_target)**2
            loss_intensity = (singularity_pred['max_intensity'][0] - max_intensity_target)**2
            loss_energy = (singularity_pred['total_energy'][0] - total_energy_target)**2

            total_singularity_loss = loss_n_sing + loss_intensity + 0.1 * loss_energy

            return total_singularity_loss, {
                'n_singularities': loss_n_sing.item(),
                'max_intensity': loss_intensity.item(),
                'total_energy': loss_energy.item(),
                'targets': {
                    'n_singularities': n_singularities_target,
                    'max_intensity': max_intensity_target,
                    'total_energy': total_energy_target
                }
            }
        except:
            return torch.tensor(0.0), {}

    def train_on_real_data(self, case_name, subdir=None, n_epochs=200, time_steps=3):
        """
        Entra√Ænement sur les vraies donn√©es Kelvin-Helmholtz
        """
        print(f"üöÄ ENTRA√éNEMENT PINNs MHD SUR VRAIES DONN√âES: {case_name}")
        print("="*70)

        # Charger les vraies donn√©es
        training_data = self.data_loader.get_training_data(case_name, subdir, time_steps)

        if not training_data:
            raise ValueError(f"Aucune donn√©e d'entra√Ænement disponible pour {case_name}")

        print(f"üìä D√âBUT ENTRA√éNEMENT:")
        print(f"   ‚Ä¢ Cas: {case_name}")
        print(f"   ‚Ä¢ Sous-dossier: {subdir}")
        print(f"   ‚Ä¢ Pas de temps: {len(training_data)}")
        print(f"   ‚Ä¢ Points par pas: {training_data[0]['coords'].shape[0]}")
        print(f"   ‚Ä¢ Champs: {list(training_data[0]['fields'].keys())}")

        start_time = time.time()

        for epoch in range(n_epochs):
            epoch_loss = 0.0
            epoch_details = {'data': 0.0, 'singularity': 0.0}

            # Entra√Æner sur chaque pas de temps
            for time_step, data_dict in enumerate(training_data):
                coords = data_dict['coords']
                fields = data_dict['fields']

                # Sous-√©chantillonner pour acc√©l√©rer l'entra√Ænement
                n_points = min(1000, coords.shape[0])
                indices = torch.randperm(coords.shape[0])[:n_points]
                coords_batch = coords[indices]
                fields_batch = {key: value[indices] if value.ndim > 1 else value 
                               for key, value in fields.items()}

                # Loss sur les donn√©es r√©elles
                data_loss, data_details = self.compute_real_data_loss(coords_batch, fields_batch)

                # Loss sur les singularit√©s (si vorticit√© disponible)
                singularity_loss = torch.tensor(0.0)
                vorticity_fields = ['vorticity', 'vorticity_computed', 'current']
                for vort_field in vorticity_fields:
                    if vort_field in fields:
                        singularity_loss, sing_details = self.compute_singularity_loss_from_real_data(
                            fields[vort_field]
                        )
                        break

                # Loss totale
                total_loss = data_loss + 0.3 * singularity_loss

                # Optimisation
                self.optimizer.zero_grad()
                total_loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                epoch_loss += total_loss.item()
                epoch_details['data'] += data_loss.item()
                epoch_details['singularity'] += singularity_loss.item()

            # Moyenner sur les pas de temps
            epoch_loss /= len(training_data)
            epoch_details['data'] /= len(training_data)
            epoch_details['singularity'] /= len(training_data)

            # Enregistrer historique
            self.loss_history['total'].append(epoch_loss)
            self.loss_history['data'].append(epoch_details['data'])
            self.loss_history['singularity'].append(epoch_details['singularity'])

            # Affichage p√©riodique
            if epoch % 25 == 0:
                elapsed = time.time() - start_time
                print(f"Epoch {epoch:3d} | Total: {epoch_loss:.4f} | "
                      f"Data: {epoch_details['data']:.4f} | "
                      f"Singularity: {epoch_details['singularity']:.4f} | "
                      f"Time: {elapsed:.1f}s")

        total_time = time.time() - start_time
        print(f"\n‚úÖ ENTRA√éNEMENT TERMIN√â en {total_time:.1f}s")

        return self.loss_history

def create_ultra_robust_mhd_demo():
    """
    D√©monstration compl√®te avec vraies donn√©es Kelvin-Helmholtz - VERSION ULTRA-ROBUSTE
    """
    print("üß† CR√âATION PINNs MHD AVEC VRAIES DONN√âES KELVIN-HELMHOLTZ - ULTRA-ROBUSTE")
    print("="*90)

    # Cr√©er le chargeur de donn√©es ultra-robuste
    data_loader = UltraRobust_RealDataLoader()

    # Cr√©er le mod√®le
    model = UltraRobust_MHD_PINNs(input_dim=2, hidden_dims=[128, 128, 64], n_scales=15)

    print(f"üìä ARCHITECTURE DU R√âSEAU:")
    print(f"   ‚Ä¢ R√©seau principal: [2] ‚Üí [128, 128, 64] ‚Üí [4 t√™tes MHD]")
    print(f"   ‚Ä¢ Ondelettes Morlet: 15 √©chelles apprenables")
    print(f"   ‚Ä¢ R√©seau singularit√©s: [45] ‚Üí [64, 32, 16] ‚Üí [3]")
    print(f"   ‚Ä¢ Param√®tres totaux: {sum(p.numel() for p in model.parameters()):,}")

    # Cr√©er l'entra√Æneur
    trainer = UltraRobust_MHD_Trainer(model, data_loader)

    # Tester le chargement des donn√©es
    print(f"\nüîç TEST CHARGEMENT DONN√âES...")
    try:
        # Essayer de charger le cas B_0 (premier cas)
        case_data = data_loader.load_case_data('B_0', max_files=3)

        if case_data:
            print(f"‚úÖ DONN√âES CHARG√âES AVEC SUCC√àS!")

            # Choisir le premier sous-dossier disponible
            first_subdir = list(case_data.keys())[0]

            # Pr√©parer les donn√©es d'entra√Ænement
            training_data = data_loader.get_training_data('B_0', first_subdir, 3)

            if training_data:
                print(f"‚úÖ DONN√âES D'ENTRA√éNEMENT PR√âPAR√âES!")

                # Entra√Ænement sur vraies donn√©es
                print(f"\nüèãÔ∏è D√âBUT ENTRA√éNEMENT SUR VRAIES DONN√âES...")
                loss_history = trainer.train_on_real_data(
                    case_name='B_0',
                    subdir=first_subdir,
                    n_epochs=200,
                    time_steps=3
                )

                return model, trainer, loss_history
            else:
                print("‚ùå √âCHEC PR√âPARATION DONN√âES D'ENTRA√éNEMENT")
                return None, None, None
        else:
            print("‚ùå AUCUNE DONN√âE CHARG√âE")
            return None, None, None

    except Exception as e:
        print(f"‚ùå ERREUR CHARGEMENT: {e}")
        import traceback
        traceback.print_exc()
        print("‚ö†Ô∏è V√©rifier les chemins d'acc√®s aux donn√©es")
        return None, None, None

# Code principal
if __name__ == "__main__":
    # Cr√©er et entra√Æner le mod√®le avec vraies donn√©es
    model, trainer, loss_history = create_ultra_robust_mhd_demo()

    if model is not None:
        print("\nüéâ ENTRA√éNEMENT SUR VRAIES DONN√âES TERMIN√â!")
        print("="*60)
        print("‚úÖ CAPACIT√âS DU R√âSEAU:")
        print("   ‚Ä¢ Entra√Æn√© sur tes vraies donn√©es Kelvin-Helmholtz")
        print("   ‚Ä¢ Pr√©diction des champs MHD r√©els")
        print("   ‚Ä¢ D√©tection de singularit√©s dans vraies donn√©es")
        print("   ‚Ä¢ Analyse par ondelettes Morlet des vrais signaux")

        # Test final
        print("\nüß™ TEST FINAL DU MOD√àLE...")
        model.eval()
        with torch.no_grad():
            # Test sur une grille
            x = torch.linspace(-1, 1, 32)
            y = torch.linspace(-1, 1, 32)
            X, Y = torch.meshgrid(x, y, indexing='ij')
            coords_test = torch.stack([X.flatten(), Y.flatten()], dim=1)

            outputs = model(coords_test)
            print(f"‚úÖ Pr√©diction r√©ussie sur {coords_test.shape[0]} points")
            print(f"   ‚Ä¢ Vitesse: {outputs['velocity'].shape}")
            print(f"   ‚Ä¢ Champ magn√©tique: {outputs['magnetic'].shape}")
            print(f"   ‚Ä¢ Pression: {outputs['pressure'].shape}")
            print(f"   ‚Ä¢ Densit√©: {outputs['density'].shape}")

            # Test singularit√©s
            vorticity_test = torch.randn(1, 64)  # Signal test
            sing_pred = model.predict_singularities(vorticity_test)
            print(f"‚úÖ Pr√©diction singularit√©s:")
            print(f"   ‚Ä¢ Nombre: {sing_pred['n_singularities'].item():.2f}")
            print(f"   ‚Ä¢ Intensit√© max: {sing_pred['max_intensity'].item():.4f}")
            print(f"   ‚Ä¢ √ânergie totale: {sing_pred['total_energy'].item():.4f}")
    else:
        print("\n‚ùå √âCHEC ENTRA√éNEMENT SUR VRAIES DONN√âES")
        print("V√©rifier les chemins d'acc√®s et la structure des donn√©es")

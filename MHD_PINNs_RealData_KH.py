
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from scipy import signal
import scipy.io as sio
import pywt
from sklearn.preprocessing import StandardScaler
import time
import os
import glob

class RealDataLoader:
    """
    Chargeur pour les vraies donn√©es Kelvin-Helmholtz - VERSION CORRIG√âE
    """
    def __init__(self, data_path="C:/Users/user/Desktop/Donnees_KVI"):
        self.data_path = data_path
        self.cases = {
            'B_0': 'Kelvin_Helmholtz_Instabilities_B_0',
            'B_0_1': 'Kelvin_Helmholtz_Instabilities_B_0_1', 
            'B_0_2': 'Kelvin_Helmholtz_Instabilities_B_0_2',
            'B_0_05': 'Kelvin_Helmholtz_Instabilities_B_0_05'
        }
        self.loaded_data = {}

    def load_case_data(self, case_name, max_files=5):
        """
        Charge les donn√©es d'un cas sp√©cifique - VERSION ROBUSTE
        """
        print(f"üîÑ CHARGEMENT DES DONN√âES: {case_name}")

        case_path = os.path.join(self.data_path, self.cases[case_name])

        # Chercher les sous-dossiers
        subdirs = []
        if os.path.exists(case_path):
            subdirs = [d for d in os.listdir(case_path) if os.path.isdir(os.path.join(case_path, d))]

        case_data = {}

        for subdir in subdirs:
            subdir_path = os.path.join(case_path, subdir)

            # Patterns de fichiers plus flexibles
            patterns = [
                os.path.join(subdir_path, "*.mat"),
                os.path.join(subdir_path, "MHD_data_*.mat"),
                os.path.join(subdir_path, f"*{case_name}*.mat")
            ]

            mat_files = []
            for pattern in patterns:
                mat_files.extend(glob.glob(pattern))

            # Supprimer les doublons et trier
            mat_files = sorted(list(set(mat_files)))[:max_files]

            subdir_data = []
            for mat_file in mat_files:
                try:
                    print(f"   üìÇ Lecture: {os.path.basename(mat_file)}")
                    data = sio.loadmat(mat_file)

                    # Afficher les cl√©s disponibles pour debug
                    available_keys = [k for k in data.keys() if not k.startswith('__')]
                    print(f"      üîë Cl√©s disponibles: {available_keys}")

                    # Extraire les champs avec noms flexibles
                    fields = {}

                    # Mapping des noms possibles
                    field_mappings = {
                        'vx': ['vx', 'u', 'velocity_x', 'vel_x'],
                        'vy': ['vy', 'v', 'velocity_y', 'vel_y'],
                        'Bx': ['Bx', 'B_x', 'magnetic_x', 'mag_x'],
                        'By': ['By', 'B_y', 'magnetic_y', 'mag_y'],
                        'rho': ['rho', 'density', 'dens'],
                        'p': ['p', 'pressure', 'pres']
                    }

                    for field_name, possible_names in field_mappings.items():
                        for name in possible_names:
                            if name in data:
                                fields[field_name] = data[name]
                                print(f"      ‚úÖ {field_name}: {data[name].shape}")
                                break

                    # Si pas de champs trouv√©s, essayer les premi√®res cl√©s num√©riques
                    if not fields and available_keys:
                        print(f"      üîÑ Tentative avec cl√©s g√©n√©riques...")
                        numeric_keys = []
                        for key in available_keys:
                            if isinstance(data[key], np.ndarray) and data[key].ndim >= 2:
                                numeric_keys.append(key)

                        # Assigner les premi√®res cl√©s trouv√©es
                        field_names = ['vx', 'vy', 'Bx', 'By', 'rho', 'p']
                        for i, key in enumerate(numeric_keys[:len(field_names)]):
                            fields[field_names[i]] = data[key]
                            print(f"      ‚úÖ {field_names[i]} <- {key}: {data[key].shape}")

                    # Calculer la vorticit√© si possible
                    if 'vx' in fields and 'vy' in fields:
                        vx, vy = fields['vx'], fields['vy']
                        if vx.ndim == 2 and vy.ndim == 2 and vx.shape == vy.shape:
                            # Calcul num√©rique de la vorticit√©
                            dvx_dy = np.gradient(vx, axis=0)
                            dvy_dx = np.gradient(vy, axis=1)
                            fields['vorticity'] = dvy_dx - dvx_dy
                            print(f"      ‚úÖ vorticity calcul√©e: {fields['vorticity'].shape}")

                    # M√©tadonn√©es
                    fields['file'] = os.path.basename(mat_file)
                    fields['time'] = self.extract_time_from_filename(mat_file)
                    fields['shape'] = list(fields.values())[0].shape if fields else None

                    if fields:
                        subdir_data.append(fields)
                        print(f"      ‚úÖ Donn√©es extraites avec succ√®s")
                    else:
                        print(f"      ‚ö†Ô∏è Aucun champ reconnu")

                except Exception as e:
                    print(f"      ‚ùå Erreur: {e}")
                    continue

            if subdir_data:
                case_data[subdir] = subdir_data
                print(f"   ‚úÖ {subdir}: {len(subdir_data)} fichiers trait√©s")

        self.loaded_data[case_name] = case_data
        return case_data

    def extract_time_from_filename(self, filename):
        """
        Extrait le temps du nom de fichier
        """
        import re
        match = re.search(r't(\d+(?:\.\d+)?)', filename)
        return float(match.group(1)) if match else 0.0

    def get_training_data(self, case_name, subdir=None, time_steps=None):
        """
        Pr√©pare les donn√©es pour l'entra√Ænement PINNs - VERSION ROBUSTE
        """
        if case_name not in self.loaded_data:
            self.load_case_data(case_name)

        case_data = self.loaded_data[case_name]

        if not case_data:
            print(f"‚ùå Aucune donn√©e disponible pour {case_name}")
            return []

        # S√©lectionner le sous-dossier
        if subdir is None:
            subdir = list(case_data.keys())[0]

        if subdir not in case_data:
            print(f"‚ùå Sous-dossier {subdir} non trouv√©")
            return []

        data_list = case_data[subdir]

        if not data_list:
            print(f"‚ùå Aucune donn√©e dans {subdir}")
            return []

        # S√©lectionner les pas de temps
        if time_steps is not None:
            data_list = data_list[:time_steps]

        # Convertir en tenseurs PyTorch
        training_data = []

        for data_dict in data_list:
            try:
                # Trouver un champ de r√©f√©rence pour les dimensions
                ref_field = None
                for key in ['vx', 'vy', 'Bx', 'By', 'rho', 'p']:
                    if key in data_dict and isinstance(data_dict[key], np.ndarray):
                        ref_field = data_dict[key]
                        break

                if ref_field is None:
                    print(f"‚ö†Ô∏è Aucun champ de r√©f√©rence trouv√©")
                    continue

                # Cr√©er la grille de coordonn√©es
                if ref_field.ndim == 2:
                    ny, nx = ref_field.shape
                    x = np.linspace(-1, 1, nx)
                    y = np.linspace(-1, 1, ny)
                    X, Y = np.meshgrid(x, y)
                    coords = np.stack([X.flatten(), Y.flatten()], axis=1)

                    # Pr√©parer les champs
                    fields = {}

                    # Vitesse
                    if 'vx' in data_dict and 'vy' in data_dict:
                        vx_flat = data_dict['vx'].flatten()
                        vy_flat = data_dict['vy'].flatten()
                        fields['velocity'] = np.stack([vx_flat, vy_flat], axis=1)

                    # Champ magn√©tique
                    if 'Bx' in data_dict and 'By' in data_dict:
                        Bx_flat = data_dict['Bx'].flatten()
                        By_flat = data_dict['By'].flatten()
                        fields['magnetic'] = np.stack([Bx_flat, By_flat], axis=1)

                    # Scalaires
                    if 'rho' in data_dict:
                        fields['density'] = data_dict['rho'].flatten().reshape(-1, 1)

                    if 'p' in data_dict:
                        fields['pressure'] = data_dict['p'].flatten().reshape(-1, 1)

                    if 'vorticity' in data_dict:
                        fields['vorticity'] = data_dict['vorticity'].flatten()

                    # Convertir en tenseurs
                    coords_tensor = torch.FloatTensor(coords)
                    fields_tensor = {}
                    for key, value in fields.items():
                        if isinstance(value, np.ndarray):
                            fields_tensor[key] = torch.FloatTensor(value)

                    if fields_tensor:  # Seulement si on a des champs
                        training_data.append({
                            'coords': coords_tensor,
                            'fields': fields_tensor,
                            'time': data_dict['time'],
                            'file': data_dict['file']
                        })
                        print(f"‚úÖ Donn√©es pr√©par√©es: {coords_tensor.shape[0]} points")

            except Exception as e:
                print(f"‚ùå Erreur pr√©paration donn√©es: {e}")
                continue

        print(f"üìä R√âSUM√â DONN√âES D'ENTRA√éNEMENT:")
        print(f"   ‚Ä¢ Cas: {case_name}")
        print(f"   ‚Ä¢ Sous-dossier: {subdir}")
        print(f"   ‚Ä¢ Pas de temps: {len(training_data)}")
        if training_data:
            print(f"   ‚Ä¢ Points par pas: {training_data[0]['coords'].shape[0]}")
            print(f"   ‚Ä¢ Champs disponibles: {list(training_data[0]['fields'].keys())}")

        return training_data

class MorletWaveletLayer(nn.Module):
    """
    Couche d'ondelettes Morlet int√©gr√©e dans le r√©seau
    """
    def __init__(self, input_dim, n_scales=15, trainable_scales=True):
        super().__init__()
        self.input_dim = input_dim
        self.n_scales = n_scales

        # √âchelles d'ondelettes (param√®tres apprenables)
        if trainable_scales:
            self.scales = nn.Parameter(torch.linspace(1, 20, n_scales))
        else:
            self.register_buffer('scales', torch.linspace(1, 20, n_scales))

        # Param√®tres Morlet apprenables
        self.omega0 = nn.Parameter(torch.tensor(6.0))  # Fr√©quence centrale
        self.sigma = nn.Parameter(torch.tensor(1.0))   # Largeur

    def morlet_wavelet(self, t, scale):
        """Ondelette Morlet complexe"""
        # Normalisation
        norm = 1.0 / torch.sqrt(scale)

        # Argument normalis√©
        arg = t / scale

        # Ondelette Morlet complexe
        envelope = torch.exp(-0.5 * (arg / self.sigma)**2)
        oscillation = torch.exp(1j * self.omega0 * arg)

        return norm * envelope * oscillation

    def forward(self, x):
        """
        Transform√©e en ondelettes continue pour chaque signal
        x: (batch_size, signal_length)
        """
        batch_size, signal_length = x.shape

        # Cr√©er la grille temporelle
        t = torch.linspace(-signal_length//2, signal_length//2, signal_length, device=x.device)

        # Coefficients d'ondelettes pour toutes les √©chelles
        coeffs = []

        for scale in self.scales:
            # Ondelette m√®re √† cette √©chelle
            wavelet = self.morlet_wavelet(t, scale)

            # Convolution (transform√©e en ondelettes)
            # Utiliser la partie r√©elle pour la suite
            wavelet_real = wavelet.real.unsqueeze(0).unsqueeze(0)  # (1, 1, signal_length)
            x_expanded = x.unsqueeze(1)  # (batch_size, 1, signal_length)

            # Convolution 1D
            coeff = torch.nn.functional.conv1d(
                x_expanded, 
                wavelet_real, 
                padding=signal_length//2
            )[:, 0, :signal_length]  # (batch_size, signal_length)

            coeffs.append(coeff)

        # Empiler tous les coefficients
        wavelet_coeffs = torch.stack(coeffs, dim=1)  # (batch_size, n_scales, signal_length)

        return wavelet_coeffs

class RealData_MHD_PINNs(nn.Module):
    """
    Physics-Informed Neural Network pour MHD avec vraies donn√©es Kelvin-Helmholtz
    """
    def __init__(self, input_dim=2, hidden_dims=[128, 128, 64], n_scales=15):
        super().__init__()

        # Couche d'ondelettes Morlet
        self.wavelet_layer = MorletWaveletLayer(input_dim, n_scales)
        self.n_scales = n_scales

        # R√©seau principal pour les champs MHD
        layers = []
        current_dim = input_dim  # (x, y) coordinates

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.Tanh(),  # Tanh pour PINNs (meilleure stabilit√©)
            ])
            current_dim = hidden_dim

        self.main_network = nn.Sequential(*layers)

        # T√™tes de sortie pour les champs MHD
        self.velocity_head = nn.Sequential(
            nn.Linear(current_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 2)  # (vx, vy)
        )

        self.magnetic_head = nn.Sequential(
            nn.Linear(current_dim, 32),
            nn.Tanh(),
            nn.Linear(32, 2)  # (Bx, By)
        )

        self.pressure_head = nn.Sequential(
            nn.Linear(current_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1)  # p
        )

        self.density_head = nn.Sequential(
            nn.Linear(current_dim, 16),
            nn.Tanh(),
            nn.Linear(16, 1)  # œÅ
        )

        # R√©seau pour pr√©diction des singularit√©s bas√© sur ondelettes
        wavelet_feature_dim = 3 * n_scales  # mean, std, max pour chaque √©chelle

        self.singularity_network = nn.Sequential(
            nn.Linear(wavelet_feature_dim, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 3)  # (n_singularities, max_intensity, total_energy)
        )

    def forward(self, coords):
        """
        coords: (batch_size, 2) - coordonn√©es (x, y)
        """
        # Extraction des features du r√©seau principal
        features = self.main_network(coords)

        # Pr√©diction des champs MHD
        velocity = self.velocity_head(features)      # (vx, vy)
        magnetic = self.magnetic_head(features)      # (Bx, By)
        pressure = self.pressure_head(features)      # p
        density = self.density_head(features)        # œÅ

        return {
            'velocity': velocity,
            'magnetic': magnetic,
            'pressure': pressure,
            'density': density,
            'features': features
        }

    def predict_singularities(self, field_data):
        """
        Pr√©dire les singularit√©s √† partir des donn√©es de champ
        field_data: (batch_size, signal_length) - donn√©es 1D du champ
        """
        # Analyse par ondelettes Morlet
        wavelet_coeffs = self.wavelet_layer(field_data)  # (batch_size, n_scales, signal_length)

        # Statistiques des coefficients d'ondelettes par √©chelle
        mean_coeffs = torch.mean(torch.abs(wavelet_coeffs), dim=2)  # (batch_size, n_scales)
        std_coeffs = torch.std(torch.abs(wavelet_coeffs), dim=2)    # (batch_size, n_scales)
        max_coeffs = torch.max(torch.abs(wavelet_coeffs), dim=2)[0] # (batch_size, n_scales)

        # Concat√©ner toutes les features: 3 * n_scales
        wavelet_features = torch.cat([mean_coeffs, std_coeffs, max_coeffs], dim=1)

        # Pr√©diction des singularit√©s
        singularity_pred = self.singularity_network(wavelet_features)

        return {
            'n_singularities': singularity_pred[:, 0],
            'max_intensity': singularity_pred[:, 1],
            'total_energy': singularity_pred[:, 2],
            'wavelet_coeffs': wavelet_coeffs
        }

class RealData_MHD_Trainer:
    """
    Entra√Æneur pour le r√©seau PINNs MHD avec vraies donn√©es - VERSION ROBUSTE
    """
    def __init__(self, model, data_loader):
        self.model = model
        self.data_loader = data_loader

        # Optimizers
        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)

        # Historique des losses
        self.loss_history = {
            'total': [], 'data': [], 'singularity': []
        }

    def compute_real_data_loss(self, coords, target_fields):
        """
        Calcule la loss bas√©e sur les vraies donn√©es
        """
        outputs = self.model(coords)

        total_loss = 0.0
        loss_details = {}
        n_losses = 0

        # Loss pour chaque champ disponible
        if 'velocity' in target_fields and 'velocity' in outputs:
            loss_velocity = torch.mean((outputs['velocity'] - target_fields['velocity'])**2)
            total_loss += loss_velocity
            loss_details['velocity'] = loss_velocity.item()
            n_losses += 1

        if 'magnetic' in target_fields and 'magnetic' in outputs:
            loss_magnetic = torch.mean((outputs['magnetic'] - target_fields['magnetic'])**2)
            total_loss += loss_magnetic
            loss_details['magnetic'] = loss_magnetic.item()
            n_losses += 1

        if 'pressure' in target_fields and 'pressure' in outputs:
            loss_pressure = torch.mean((outputs['pressure'] - target_fields['pressure'])**2)
            total_loss += loss_pressure
            loss_details['pressure'] = loss_pressure.item()
            n_losses += 1

        if 'density' in target_fields and 'density' in outputs:
            loss_density = torch.mean((outputs['density'] - target_fields['density'])**2)
            total_loss += loss_density
            loss_details['density'] = loss_density.item()
            n_losses += 1

        # Moyenner si plusieurs losses
        if n_losses > 0:
            total_loss = total_loss / n_losses

        return total_loss, loss_details

    def compute_singularity_loss_from_real_data(self, real_vorticity):
        """
        Calcule la loss des singularit√©s √† partir de vraies donn√©es de vorticit√©
        """
        if real_vorticity is None or len(real_vorticity) == 0:
            return torch.tensor(0.0), {}

        # Analyser les vraies donn√©es pour extraire les singularit√©s
        vorticity_1d = real_vorticity.flatten()

        # D√©tection simple des singularit√©s (pics dans la vorticit√©)
        vorticity_np = vorticity_1d.detach().cpu().numpy()

        # Trouver les pics
        try:
            from scipy.signal import find_peaks
            peaks, properties = find_peaks(np.abs(vorticity_np), height=np.std(vorticity_np))

            # Targets bas√©s sur les vraies donn√©es
            n_singularities_target = len(peaks)
            max_intensity_target = np.max(np.abs(vorticity_np)) if len(vorticity_np) > 0 else 0.0
            total_energy_target = np.sum(vorticity_np**2)

            # Pr√©diction du mod√®le
            vorticity_tensor = vorticity_1d.unsqueeze(0)  # (1, signal_length)
            singularity_pred = self.model.predict_singularities(vorticity_tensor)

            # Loss
            loss_n_sing = (singularity_pred['n_singularities'][0] - n_singularities_target)**2
            loss_intensity = (singularity_pred['max_intensity'][0] - max_intensity_target)**2
            loss_energy = (singularity_pred['total_energy'][0] - total_energy_target)**2

            total_singularity_loss = loss_n_sing + loss_intensity + 0.1 * loss_energy

            return total_singularity_loss, {
                'n_singularities': loss_n_sing.item(),
                'max_intensity': loss_intensity.item(),
                'total_energy': loss_energy.item(),
                'targets': {
                    'n_singularities': n_singularities_target,
                    'max_intensity': max_intensity_target,
                    'total_energy': total_energy_target
                }
            }
        except:
            return torch.tensor(0.0), {}

    def train_on_real_data(self, case_name, subdir=None, n_epochs=300, time_steps=3):
        """
        Entra√Ænement sur les vraies donn√©es Kelvin-Helmholtz
        """
        print(f"üöÄ ENTRA√éNEMENT PINNs MHD SUR VRAIES DONN√âES: {case_name}")
        print("="*70)

        # Charger les vraies donn√©es
        training_data = self.data_loader.get_training_data(case_name, subdir, time_steps)

        if not training_data:
            raise ValueError(f"Aucune donn√©e d'entra√Ænement disponible pour {case_name}")

        print(f"üìä D√âBUT ENTRA√éNEMENT:")
        print(f"   ‚Ä¢ Cas: {case_name}")
        print(f"   ‚Ä¢ Sous-dossier: {subdir}")
        print(f"   ‚Ä¢ Pas de temps: {len(training_data)}")
        print(f"   ‚Ä¢ Points par pas: {training_data[0]['coords'].shape[0]}")
        print(f"   ‚Ä¢ Champs: {list(training_data[0]['fields'].keys())}")

        start_time = time.time()

        for epoch in range(n_epochs):
            epoch_loss = 0.0
            epoch_details = {'data': 0.0, 'singularity': 0.0}

            # Entra√Æner sur chaque pas de temps
            for time_step, data_dict in enumerate(training_data):
                coords = data_dict['coords']
                fields = data_dict['fields']

                # Sous-√©chantillonner pour acc√©l√©rer l'entra√Ænement
                n_points = min(800, coords.shape[0])
                indices = torch.randperm(coords.shape[0])[:n_points]
                coords_batch = coords[indices]
                fields_batch = {key: value[indices] if value.ndim > 1 else value 
                               for key, value in fields.items()}

                # Loss sur les donn√©es r√©elles
                data_loss, data_details = self.compute_real_data_loss(coords_batch, fields_batch)

                # Loss sur les singularit√©s (si vorticit√© disponible)
                singularity_loss = torch.tensor(0.0)
                if 'vorticity' in fields:
                    singularity_loss, sing_details = self.compute_singularity_loss_from_real_data(
                        fields['vorticity']
                    )

                # Loss totale
                total_loss = data_loss + 0.5 * singularity_loss

                # Optimisation
                self.optimizer.zero_grad()
                total_loss.backward()

                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                self.optimizer.step()

                epoch_loss += total_loss.item()
                epoch_details['data'] += data_loss.item()
                epoch_details['singularity'] += singularity_loss.item()

            # Moyenner sur les pas de temps
            epoch_loss /= len(training_data)
            epoch_details['data'] /= len(training_data)
            epoch_details['singularity'] /= len(training_data)

            # Enregistrer historique
            self.loss_history['total'].append(epoch_loss)
            self.loss_history['data'].append(epoch_details['data'])
            self.loss_history['singularity'].append(epoch_details['singularity'])

            # Affichage p√©riodique
            if epoch % 50 == 0:
                elapsed = time.time() - start_time
                print(f"Epoch {epoch:3d} | Total: {epoch_loss:.4f} | "
                      f"Data: {epoch_details['data']:.4f} | "
                      f"Singularity: {epoch_details['singularity']:.4f} | "
                      f"Time: {elapsed:.1f}s")

        total_time = time.time() - start_time
        print(f"\n‚úÖ ENTRA√éNEMENT TERMIN√â en {total_time:.1f}s")

        return self.loss_history

def create_real_data_mhd_demo():
    """
    D√©monstration compl√®te avec vraies donn√©es Kelvin-Helmholtz - VERSION ROBUSTE
    """
    print("üß† CR√âATION PINNs MHD AVEC VRAIES DONN√âES KELVIN-HELMHOLTZ")
    print("="*80)

    # Cr√©er le chargeur de donn√©es
    data_loader = RealDataLoader()

    # Cr√©er le mod√®le
    model = RealData_MHD_PINNs(input_dim=2, hidden_dims=[128, 128, 64], n_scales=15)

    print(f"üìä ARCHITECTURE DU R√âSEAU:")
    print(f"   ‚Ä¢ R√©seau principal: [2] ‚Üí [128, 128, 64] ‚Üí [4 t√™tes MHD]")
    print(f"   ‚Ä¢ Ondelettes Morlet: 15 √©chelles apprenables")
    print(f"   ‚Ä¢ R√©seau singularit√©s: [45] ‚Üí [64, 32, 16] ‚Üí [3]")
    print(f"   ‚Ä¢ Param√®tres totaux: {sum(p.numel() for p in model.parameters()):,}")

    # Cr√©er l'entra√Æneur
    trainer = RealData_MHD_Trainer(model, data_loader)

    # Tester le chargement des donn√©es
    print(f"\nüîç TEST CHARGEMENT DONN√âES...")
    try:
        # Essayer de charger le cas B_0 (premier cas)
        case_data = data_loader.load_case_data('B_0', max_files=3)

        if case_data:
            print(f"‚úÖ DONN√âES CHARG√âES AVEC SUCC√àS!")

            # Choisir le premier sous-dossier disponible
            first_subdir = list(case_data.keys())[0]

            # Pr√©parer les donn√©es d'entra√Ænement
            training_data = data_loader.get_training_data('B_0', first_subdir, 3)

            if training_data:
                print(f"‚úÖ DONN√âES D'ENTRA√éNEMENT PR√âPAR√âES!")

                # Entra√Ænement sur vraies donn√©es
                print(f"\nüèãÔ∏è D√âBUT ENTRA√éNEMENT SUR VRAIES DONN√âES...")
                loss_history = trainer.train_on_real_data(
                    case_name='B_0',
                    subdir=first_subdir,
                    n_epochs=300,
                    time_steps=3
                )

                return model, trainer, loss_history
            else:
                print("‚ùå √âCHEC PR√âPARATION DONN√âES D'ENTRA√éNEMENT")
                return None, None, None
        else:
            print("‚ùå AUCUNE DONN√âE CHARG√âE")
            return None, None, None

    except Exception as e:
        print(f"‚ùå ERREUR CHARGEMENT: {e}")
        print("‚ö†Ô∏è V√©rifier les chemins d'acc√®s aux donn√©es")
        return None, None, None

# Code principal
if __name__ == "__main__":
    # Cr√©er et entra√Æner le mod√®le avec vraies donn√©es
    model, trainer, loss_history = create_real_data_mhd_demo()

    if model is not None:
        print("\nüéâ ENTRA√éNEMENT SUR VRAIES DONN√âES TERMIN√â!")
        print("="*60)
        print("‚úÖ CAPACIT√âS DU R√âSEAU:")
        print("   ‚Ä¢ Entra√Æn√© sur tes vraies donn√©es Kelvin-Helmholtz")
        print("   ‚Ä¢ Pr√©diction des champs MHD r√©els")
        print("   ‚Ä¢ D√©tection de singularit√©s dans vraies donn√©es")
        print("   ‚Ä¢ Analyse par ondelettes Morlet des vrais signaux")

        # Test final
        print("\nüß™ TEST FINAL DU MOD√àLE...")
        model.eval()
        with torch.no_grad():
            # Test sur une grille
            x = torch.linspace(-1, 1, 32)
            y = torch.linspace(-1, 1, 32)
            X, Y = torch.meshgrid(x, y, indexing='ij')
            coords_test = torch.stack([X.flatten(), Y.flatten()], dim=1)

            outputs = model(coords_test)
            print(f"‚úÖ Pr√©diction r√©ussie sur {coords_test.shape[0]} points")
            print(f"   ‚Ä¢ Vitesse: {outputs['velocity'].shape}")
            print(f"   ‚Ä¢ Champ magn√©tique: {outputs['magnetic'].shape}")
            print(f"   ‚Ä¢ Pression: {outputs['pressure'].shape}")
            print(f"   ‚Ä¢ Densit√©: {outputs['density'].shape}")

            # Test singularit√©s
            vorticity_test = torch.randn(1, 64)  # Signal test
            sing_pred = model.predict_singularities(vorticity_test)
            print(f"‚úÖ Pr√©diction singularit√©s:")
            print(f"   ‚Ä¢ Nombre: {sing_pred['n_singularities'].item():.2f}")
            print(f"   ‚Ä¢ Intensit√© max: {sing_pred['max_intensity'].item():.4f}")
            print(f"   ‚Ä¢ √ânergie totale: {sing_pred['total_energy'].item():.4f}")
    else:
        print("\n‚ùå √âCHEC ENTRA√éNEMENT SUR VRAIES DONN√âES")
        print("V√©rifier les chemins d'acc√®s et la structure des donn√©es")

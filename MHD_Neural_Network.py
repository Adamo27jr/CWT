
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from scipy import signal
import pywt

class MHD_WaveletNet(nn.Module):
    """
    R√©seau de neurones sp√©cialis√© pour MHD avec ondelettes
    Architecture: Wavelet Features ‚Üí Dense ‚Üí Pr√©dictions multiples
    """
    def __init__(self, wavelet_features=100, hidden_dims=[128, 64, 32]):
        super().__init__()

        # Couches principales
        layers = []
        input_dim = wavelet_features

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(input_dim, hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.BatchNorm1d(hidden_dim)
            ])
            input_dim = hidden_dim

        self.feature_extractor = nn.Sequential(*layers)

        # T√™tes de pr√©diction multiples
        self.singularity_head = nn.Sequential(
            nn.Linear(hidden_dims[-1], 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

        self.intensity_head = nn.Sequential(
            nn.Linear(hidden_dims[-1], 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

        self.energy_head = nn.Sequential(
            nn.Linear(hidden_dims[-1], 16),
            nn.ReLU(),
            nn.Linear(16, 1)
        )

        self.regime_head = nn.Sequential(
            nn.Linear(hidden_dims[-1], 16),
            nn.ReLU(),
            nn.Linear(16, 4)  # 4 r√©gimes: Chaos_HD, Stable, Transition, Resonance
        )

    def forward(self, x):
        features = self.feature_extractor(x)

        singularities = self.singularity_head(features)
        intensity = self.intensity_head(features)
        energy = self.energy_head(features)
        regime_logits = self.regime_head(features)

        return {
            'singularities': singularities,
            'intensity': intensity,
            'energy': energy,
            'regime': regime_logits
        }

class WaveletFeatureExtractor:
    """
    Extracteur de features ondelettes pour donn√©es MHD
    """
    def __init__(self, wavelet='morl', scales=np.arange(1, 21)):
        self.wavelet = wavelet
        self.scales = scales
        self.scaler = StandardScaler()

    def extract_features(self, data_2d):
        """
        Extrait features ondelettes d'un champ 2D
        """
        features = []

        # 1. Transform√©e ondelettes continue sur diff√©rentes directions
        # Horizontal cuts
        for i in range(0, data_2d.shape[0], max(1, data_2d.shape[0]//5)):
            line = data_2d[i, :]
            coeffs, _ = pywt.cwt(line, self.scales, self.wavelet)
            features.extend([
                np.mean(np.abs(coeffs)),
                np.std(np.abs(coeffs)),
                np.max(np.abs(coeffs)),
                len(signal.find_peaks(np.abs(coeffs.flatten()), height=np.std(np.abs(coeffs)))[0])
            ])

        # Vertical cuts
        for j in range(0, data_2d.shape[1], max(1, data_2d.shape[1]//5)):
            line = data_2d[:, j]
            coeffs, _ = pywt.cwt(line, self.scales, self.wavelet)
            features.extend([
                np.mean(np.abs(coeffs)),
                np.std(np.abs(coeffs)),
                np.max(np.abs(coeffs)),
                len(signal.find_peaks(np.abs(coeffs.flatten()), height=np.std(np.abs(coeffs)))[0])
            ])

        # 2. Features globales
        # Gradient magnitude
        grad_x, grad_y = np.gradient(data_2d)
        grad_mag = np.sqrt(grad_x**2 + grad_y**2)

        features.extend([
            np.mean(grad_mag),
            np.std(grad_mag),
            np.max(grad_mag),
            np.sum(grad_mag > 2*np.std(grad_mag))  # Points de fort gradient
        ])

        # 3. Features statistiques du champ
        features.extend([
            np.mean(data_2d),
            np.std(data_2d),
            np.min(data_2d),
            np.max(data_2d),
            np.sum(np.abs(data_2d) > 2*np.std(data_2d)),  # Outliers
            np.sum(data_2d > 0) / data_2d.size,  # Fraction positive
        ])

        return np.array(features)

    def fit_scaler(self, features_list):
        """Fit le scaler sur une liste de features"""
        all_features = np.vstack(features_list)
        self.scaler.fit(all_features)

    def transform(self, features):
        """Normalise les features"""
        return self.scaler.transform(features.reshape(1, -1)).flatten()

def create_synthetic_mhd_data(n_samples=1000):
    """
    Cr√©e des donn√©es MHD synth√©tiques pour entra√Ænement
    Bas√© sur tes d√©couvertes r√©elles
    """
    np.random.seed(42)

    data = []

    for i in range(n_samples):
        # Param√®tres al√©atoires
        B0 = np.random.uniform(0.05, 0.25)
        nx, ny = 64, 64

        # G√©n√©ration du champ de vorticit√© bas√© sur Kelvin-Helmholtz
        x = np.linspace(0, 2*np.pi, nx)
        y = np.linspace(0, 2*np.pi, ny)
        X, Y = np.meshgrid(x, y)

        # Instabilit√© KH avec influence magn√©tique
        vorticity = np.tanh((Y - np.pi)/0.1) + 0.1*np.sin(4*X)*np.exp(-(Y-np.pi)**2/0.5)

        # Effet du champ magn√©tique (bas√© sur tes d√©couvertes)
        if B0 < 0.08:
            # Chaos HD
            noise_level = 0.5
            n_sing_base = 50
        elif B0 < 0.13:
            # Stable
            noise_level = 0.1
            n_sing_base = 15
        elif B0 < 0.17:
            # Transition (paradoxe B0=0.15)
            noise_level = 0.05
            n_sing_base = 12
            vorticity *= 2  # Intensit√© plus forte
        else:
            # R√©sonance
            noise_level = 0.3
            n_sing_base = 60

        # Ajouter du bruit
        vorticity += noise_level * np.random.randn(nx, ny)

        # Calculer les targets bas√©s sur tes r√©sultats
        n_singularities = n_sing_base + np.random.randint(-5, 6)
        max_intensity = np.max(np.abs(vorticity))
        energy_growth = max_intensity * 50 * (1 + np.random.normal(0, 0.2))

        # R√©gime
        if B0 < 0.08:
            regime = 0  # Chaos_HD
        elif B0 < 0.13:
            regime = 1  # Stable
        elif B0 < 0.17:
            regime = 2  # Transition
        else:
            regime = 3  # Resonance

        data.append({
            'B0': B0,
            'vorticity': vorticity,
            'n_singularities': n_singularities,
            'max_intensity': max_intensity,
            'energy_growth': energy_growth,
            'regime': regime
        })

    return data

def train_mhd_network():
    """
    Entra√Æne le r√©seau de neurones MHD
    """
    print("üöÄ CR√âATION ET ENTRA√éNEMENT DU R√âSEAU MHD + ONDELETTES")
    print("="*60)

    # 1. G√©n√©rer donn√©es synth√©tiques
    print("üìä G√©n√©ration des donn√©es synth√©tiques...")
    synthetic_data = create_synthetic_mhd_data(1000)

    # 2. Extraire features ondelettes
    print("üåä Extraction des features ondelettes...")
    feature_extractor = WaveletFeatureExtractor()

    features_list = []
    targets = {
        'singularities': [],
        'intensity': [],
        'energy': [],
        'regime': []
    }

    for sample in synthetic_data:
        # Extraire features
        features = feature_extractor.extract_features(sample['vorticity'])
        features_list.append(features)

        # Targets
        targets['singularities'].append(sample['n_singularities'])
        targets['intensity'].append(sample['max_intensity'])
        targets['energy'].append(sample['energy_growth'])
        targets['regime'].append(sample['regime'])

    # 3. Normalisation
    feature_extractor.fit_scaler(features_list)
    X = np.array([feature_extractor.transform(f) for f in features_list])

    # 4. Pr√©parer targets
    y_sing = np.array(targets['singularities']).reshape(-1, 1)
    y_intensity = np.array(targets['intensity']).reshape(-1, 1)
    y_energy = np.array(targets['energy']).reshape(-1, 1)
    y_regime = np.array(targets['regime'])

    # 5. Split train/test
    X_train, X_test, y_sing_train, y_sing_test = train_test_split(
        X, y_sing, test_size=0.2, random_state=42
    )
    _, _, y_int_train, y_int_test = train_test_split(
        X, y_intensity, test_size=0.2, random_state=42
    )
    _, _, y_eng_train, y_eng_test = train_test_split(
        X, y_energy, test_size=0.2, random_state=42
    )
    _, _, y_reg_train, y_reg_test = train_test_split(
        X, y_regime, test_size=0.2, random_state=42
    )

    # 6. Cr√©er le mod√®le
    model = MHD_WaveletNet(wavelet_features=X.shape[1])

    # 7. Loss functions et optimizer
    mse_loss = nn.MSELoss()
    ce_loss = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # 8. Entra√Ænement
    print("üß† Entra√Ænement du r√©seau...")
    model.train()

    # Convert to tensors
    X_train_t = torch.FloatTensor(X_train)
    y_sing_train_t = torch.FloatTensor(y_sing_train)
    y_int_train_t = torch.FloatTensor(y_int_train)
    y_eng_train_t = torch.FloatTensor(y_eng_train)
    y_reg_train_t = torch.LongTensor(y_reg_train)

    losses = []

    for epoch in range(200):
        optimizer.zero_grad()

        # Forward pass
        outputs = model(X_train_t)

        # Calcul des losses
        loss_sing = mse_loss(outputs['singularities'], y_sing_train_t)
        loss_int = mse_loss(outputs['intensity'], y_int_train_t)
        loss_eng = mse_loss(outputs['energy'], y_eng_train_t)
        loss_reg = ce_loss(outputs['regime'], y_reg_train_t)

        # Loss totale
        total_loss = loss_sing + loss_int + 0.1*loss_eng + 0.5*loss_reg

        # Backward pass
        total_loss.backward()
        optimizer.step()

        losses.append(total_loss.item())

        if epoch % 50 == 0:
            print(f"Epoch {epoch}: Loss = {total_loss.item():.4f}")

    # 9. √âvaluation
    print("\nüìä √âVALUATION DU MOD√àLE")
    print("="*30)

    model.eval()
    with torch.no_grad():
        X_test_t = torch.FloatTensor(X_test)
        test_outputs = model(X_test_t)

        # M√©triques
        sing_pred = test_outputs['singularities'].numpy()
        sing_true = y_sing_test

        mse_sing = np.mean((sing_pred - sing_true)**2)
        mae_sing = np.mean(np.abs(sing_pred - sing_true))

        print(f"Singularit√©s - MSE: {mse_sing:.2f}, MAE: {mae_sing:.2f}")

        # R√©gimes
        regime_pred = torch.argmax(test_outputs['regime'], dim=1).numpy()
        regime_acc = np.mean(regime_pred == y_reg_test)
        print(f"Classification r√©gimes - Accuracy: {regime_acc:.3f}")

    # 10. Visualisation
    plt.figure(figsize=(15, 5))

    # Loss curve
    plt.subplot(1, 3, 1)
    plt.plot(losses)
    plt.title('Courbe de Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)

    # Pr√©dictions vs r√©alit√© (singularit√©s)
    plt.subplot(1, 3, 2)
    plt.scatter(sing_true, sing_pred, alpha=0.6)
    plt.plot([sing_true.min(), sing_true.max()], [sing_true.min(), sing_true.max()], 'r--')
    plt.xlabel('Singularit√©s r√©elles')
    plt.ylabel('Singularit√©s pr√©dites')
    plt.title('Pr√©diction Singularit√©s')
    plt.grid(True)

    # Matrice de confusion r√©gimes
    plt.subplot(1, 3, 3)
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_reg_test, regime_pred)
    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
    plt.title('Matrice Confusion R√©gimes')
    plt.colorbar()

    regime_names = ['Chaos_HD', 'Stable', 'Transition', 'Resonance']
    tick_marks = np.arange(len(regime_names))
    plt.xticks(tick_marks, regime_names, rotation=45)
    plt.yticks(tick_marks, regime_names)

    plt.tight_layout()
    plt.show()

    print("\n‚úÖ ENTRA√éNEMENT TERMIN√â!")
    print("üéØ Le r√©seau peut maintenant pr√©dire:")
    print("   ‚Ä¢ Nombre de singularit√©s")
    print("   ‚Ä¢ Intensit√© maximale")
    print("   ‚Ä¢ Croissance √©nerg√©tique")
    print("   ‚Ä¢ Classification du r√©gime MHD")

    return model, feature_extractor

# Exemple d'utilisation
if __name__ == "__main__":
    model, extractor = train_mhd_network()
